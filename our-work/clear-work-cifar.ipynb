{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from mlfunctions import MNISTNet, cache, do_fit,fit_optimizer, MNISTLoss, w, Optimizer\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import csv\n",
    "import copy\n",
    "import joblib\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")\n",
    "from pdb import set_trace as bp\n",
    "from meta_module import MetaLinear, MetaModule,MetaConv2d,MetaConvTranspose2d\n",
    "import functools\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "source": [
    "## CIFAR-10 training \n",
    "\n",
    "First, change the optimizer structure using the MetaConv2D. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Loss:\n",
    "    def __init__(self, training=True):\n",
    "        dataset = datasets.CIFAR10(\n",
    "            '/home/ourwork', train=True, download=True,\n",
    "            transform=torchvision.transforms.ToTensor()\n",
    "        )\n",
    "        indices = list(range(len(dataset)))\n",
    "        np.random.RandomState(10).shuffle(indices)\n",
    "        if training:\n",
    "            indices = indices[:len(indices) // 2]\n",
    "        else:\n",
    "            indices = indices[len(indices) // 2:]\n",
    "\n",
    "        self.loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=128,\n",
    "            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n",
    "\n",
    "        self.batches = []\n",
    "        self.cur_batch = 0\n",
    "        \n",
    "    def sample(self):\n",
    "        if self.cur_batch >= len(self.batches):\n",
    "            self.batches = []\n",
    "            self.cur_batch = 0\n",
    "            for b in self.loader:\n",
    "                self.batches.append(b)\n",
    "        batch = self.batches[self.cur_batch]\n",
    "        self.cur_batch += 1\n",
    "        return batch\n",
    "\n",
    "class CIFAR10Net(MetaModule):\n",
    "\n",
    "    def __init__(self, layer_size=32, n_layers=3, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # add linear layers \n",
    "        \n",
    "        # for i in range(n_layers):\n",
    "        #     self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n",
    "        #     inp_size = layer_size\n",
    "\n",
    "        self.layers = {}\n",
    "        # Main layers (Convolutions + MaxPooling)\n",
    "        in_channels = 3\n",
    "        hidden_channels = [8,16,32]\n",
    "        for i in range(n_layers):\n",
    "            self.layers[f'conv_{i}'] = MetaConv2d(in_channels, hidden_channels[i], kernel_size=5)\n",
    "            in_channels = hidden_channels[i]\n",
    "\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        # Linear Layers \n",
    "        self.layers['linear'] = MetaLinear(32, layer_size)\n",
    "\n",
    "        self.layers = nn.ModuleDict(self.layers)\n",
    "\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def all_named_parameters(self):\n",
    "        return [(k, v) for k, v in self.named_parameters()]\n",
    "    \n",
    "    def forward(self, loss):\n",
    "        inp, out = loss.sample()\n",
    "        print(inp.shape)\n",
    "\n",
    "        inp = w(Variable(inp.view(inp.size()[0], (3,32 ,32))))    # reshapes from inp.shapte\n",
    "\n",
    "        out = w(Variable(out))\n",
    "\n",
    "        \n",
    "        cur_layer = 0\n",
    "        while f'conv_{cur_layer}' in self.layers:\n",
    "            inp = self.activation(self.layers[f'conv_{cur_layer}'](inp))\n",
    "            inp = self.pool(inp)\n",
    "            cur_layer += 1\n",
    "\n",
    "        \n",
    "        inp = inp.view(-1, 32)  # flatten\n",
    "\n",
    "        inp = F.log_softmax(self.layers['linear'](inp), dim=1)\n",
    "\n",
    "        l = self.loss(inp, out)\n",
    "\n",
    "        return l"
   ]
  },
  {
   "source": [
    "After the optmizee is done, we re-build the optmizer with athe new architecture: 2 LSTMs in series - one forthe conv2Dand one for the lienear (i.e.) fully connected. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer(nn.Module):\n",
    "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "        if preproc:\n",
    "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
    "        else:\n",
    "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
    "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
    "        self.output = nn.Linear(hidden_sz, 1)\n",
    "        self.preproc = preproc\n",
    "        self.preproc_factor = preproc_factor\n",
    "        self.preproc_threshold = np.exp(-preproc_factor)\n",
    "\n",
    "    def forward(self, inp, hidden, cell):\n",
    "        if self.preproc:\n",
    "            # Implement preproc described in Appendix A\n",
    "\n",
    "            # Note: we do all this work on tensors, which means\n",
    "            # the gradients won't propagate through inp. This\n",
    "            # should be ok because the algorithm involves\n",
    "            # making sure that inp is already detached.\n",
    "            inp = inp.data\n",
    "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
    "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
    "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
    "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
    "\n",
    "            inp2[:, 0][~keep_grads] = -1\n",
    "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
    "            inp = w(Variable(inp2))\n",
    "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
    "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
    "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n"
   ]
  },
  {
   "source": [
    "Now let's try to fit CIFAR classifier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache.cache\n",
    "def get_fit_dict_test(n_tests, opt_dict, *args, **kwargs):\n",
    "    opt = w(Optimizer(preproc=True))\n",
    "    opt.load_state_dict(opt_dict)\n",
    "    np.random.seed(0)\n",
    "    return [do_fit(opt, *args, **kwargs) for _ in tqdm(range(N_TESTS), 'optimizer')]\n",
    "\n",
    "\n",
    "@cache.cache\n",
    "def fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n",
    "    results = []\n",
    "    for i in tqdm(range(n_tests), 'tests'):\n",
    "        target = target_cls(training=False)\n",
    "        optimizee = w(target_to_opt())\n",
    "        optimizer = opt_class(optimizee.parameters(), **kwargs)\n",
    "        total_loss = []\n",
    "        for _ in range(n_epochs):\n",
    "            loss = optimizee(target)\n",
    "            \n",
    "            total_loss.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        results.append(total_loss)\n",
    "    return results"
   ]
  },
  {
   "source": [
    "Optimizer training is done here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "epochs:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fc9857b5ac94bcda4b2cf476827cac4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "iterations:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91a4ceaec96441e9a35ad4cb0cc33db4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "view(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-1723804d8f81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     print('Trying lr:', lr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar10_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCIFAR10Loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCIFAR10Net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_tests\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmust_call\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m             \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                 \u001b[1;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m         self.store_backend.dump_item(\n\u001b[0;32m    763\u001b[0m             [func_id, args_id], output, verbose=self._verbose)\n",
      "\u001b[1;32mc:\\Users\\vladg\\OneDrive\\Documents\\GitHub\\learn-to-learn\\our-work\\mlfunctions.py\u001b[0m in \u001b[0;36mfit_optimizer\u001b[1;34m(target_cls, target_to_opt, preproc, unroll, optim_it, n_epochs, n_tests, lr, out_mul)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'epochs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'iterations'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mdo_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_to_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munroll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_it\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_mul\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         loss = (np.mean([\n",
      "\u001b[1;32mc:\\Users\\vladg\\OneDrive\\Documents\\GitHub\\learn-to-learn\\our-work\\mlfunctions.py\u001b[0m in \u001b[0;36mdo_fit\u001b[1;34m(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mall_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_it\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizee\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;31m#         print(f'optimizee is running on {next(optimizee.parameters()).device}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-4cddbaa3acf1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, loss)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# reshapes from inp.shapte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: view(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "# for lr in tqdm(sorted([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], key=lambda x: np.abs(x - 0.003)), 'all'):\n",
    "#     print('Trying lr:', lr)\n",
    "\n",
    "loss, cifar10_optimizer = fit_optimizer(CIFAR10Loss, CIFAR10Net, lr=0.03, preproc=False, n_tests=5, n_epochs=10)[0]"
   ]
  }
 ]
}