{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# The Trainer Notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I will start by implemetign the quadratic module. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1361210156998100"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "import csv\n",
    "import copy\n",
    "import joblib\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "cache = joblib.Memory(location='_cache', verbose=0)\n",
    "\n",
    "seed_value = 4\n",
    "torch.seed()"
   ]
  },
  {
   "source": [
    "Activate CUDA if available. Now import the two meta modules: meta_repro (currently developed) and meta_module (source)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def cuda_(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "USE_CUDA"
   ]
  },
  {
   "source": [
    "## Implementation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This is where things get interesting. Pytorch is great for implementing this paper because we have an easy way of accessing the gradients of the optimizee: simply run .backward() on its loss and get the gradient of the parameter we want by using .grad on that parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, define the OPTIMIZER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(nn.Module):\n",
    "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_sz = hidden_sz\n",
    "\n",
    "        if preproc:\n",
    "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
    "        else:\n",
    "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
    "            \n",
    "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
    "        self.output = nn.Linear(hidden_sz, 1)\n",
    "        self.preproc = preproc\n",
    "        self.preproc_factor = preproc_factor\n",
    "        self.preproc_threshold = np.exp(-preproc_factor)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        if self.preproc:\n",
    "            # Implement preproc described in Appendix A\n",
    "            \n",
    "            # Note: we do all this work on tensors, which means\n",
    "            # the gradients won't propagate through inp. This\n",
    "            # should be ok because the algorithm involves\n",
    "            # making sure that inp is already detached.\n",
    "            inp = inp.data\n",
    "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
    "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
    "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
    "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
    "            \n",
    "            inp2[:, 0][~keep_grads] = -1\n",
    "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
    "            inp = w(Variable(inp2))\n",
    "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
    "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
    "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n",
    "    "
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach_var(v):\n",
    "    var = w(Variable(v.data, requires_grad=True))\n",
    "    var.retain_grad()\n",
    "    return var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\n",
    "    if should_train:\n",
    "        opt_net.train()\n",
    "    else:\n",
    "        opt_net.eval()\n",
    "        unroll = 1\n",
    "    \n",
    "    target = target_cls(training=should_train)\n",
    "    optimizee = w(target_to_opt())\n",
    "    n_params = 0\n",
    "    for p in optimizee.parameters():\n",
    "        n_params += int(np.prod(p.size()))\n",
    "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "    all_losses_ever = []\n",
    "    if should_train:\n",
    "        meta_opt.zero_grad()\n",
    "    all_losses = None\n",
    "    for iteration in range(1, optim_it + 1):\n",
    "        loss = optimizee(target)\n",
    "                    \n",
    "        if all_losses is None:\n",
    "            all_losses = loss\n",
    "        else:\n",
    "            all_losses += loss\n",
    "        \n",
    "        all_losses_ever.append(loss.data.cpu().numpy())\n",
    "        loss.backward(retain_graph=should_train)\n",
    "\n",
    "        offset = 0\n",
    "        result_params = {}\n",
    "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\n",
    "        for name, p in optimizee.all_named_parameters():\n",
    "            cur_sz = int(np.prod(p.size()))\n",
    "            # We do this so the gradients are disconnected from the graph but we still get\n",
    "            # gradients from the rest\n",
    "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
    "            updates, new_hidden, new_cell = opt_net(\n",
    "                gradients,\n",
    "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
    "                [c[offset:offset+cur_sz] for c in cell_states]\n",
    "            )\n",
    "            for i in range(len(new_hidden)):\n",
    "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
    "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
    "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
    "            result_params[name].retain_grad()\n",
    "            \n",
    "            offset += cur_sz\n",
    "            \n",
    "        if iteration % unroll == 0:\n",
    "            if should_train:\n",
    "                meta_opt.zero_grad()\n",
    "                all_losses.backward()\n",
    "                meta_opt.step()\n",
    "                \n",
    "            all_losses = None\n",
    "                        \n",
    "            optimizee = w(target_to_opt(**{k: detach_var(v) for k, v in result_params.items()}))\n",
    "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
    "            cell_states = [detach_var(v) for v in cell_states2]\n",
    "            \n",
    "        else:\n",
    "            optimizee = w(target_to_opt(**result_params))\n",
    "            assert len(list(optimizee.all_named_parameters()))\n",
    "            hidden_states = hidden_states2\n",
    "            cell_states = cell_states2\n",
    "            \n",
    "    return all_losses_ever\n",
    "\n",
    "\n",
    "@cache.cache\n",
    "def fit_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
    "    opt_net = w(Optimizer(preproc=preproc))\n",
    "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\n",
    "    \n",
    "    best_net = None\n",
    "    best_loss = 100000000000000000\n",
    "    \n",
    "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
    "        for _ in tqdm(range(20), 'iterations'):\n",
    "            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\n",
    "        \n",
    "        loss = (np.mean([\n",
    "            np.sum(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\n",
    "            for _ in tqdm(range(n_tests), 'tests')\n",
    "        ]))\n",
    "        print(loss)\n",
    "        if loss < best_loss:\n",
    "            print(best_loss, loss)\n",
    "            best_loss = loss\n",
    "            best_net = copy.deepcopy(opt_net.state_dict())\n",
    "            \n",
    "    return best_loss, best_net"
   ]
  },
  {
   "source": [
    "## Experiment 1 -- Quadratic Loss for Linear Equation Solver"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This one consists of inversing a 10 x 10 matrix W by solving the linear equation $\\vec{y}  = W \\cdot \\vec{\\theta}$ with W being the matrix and $\\vec{\\theta}$ the vector to be found by the model.\n",
    " To asses how weel it was inverted, squarred error is used as loss. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inverse:\n",
    "    def __init__(self,dim=10):\n",
    "        self.W = cuda_(Variable(torch.rand(dim,dim)))\n",
    "        self.y = cuda_(Variable(torch.rand(dim)))\n",
    "        self.dim = (Variable(torch.tensor(dim)))\n",
    "\n",
    "    def get_loss(self,theta):\n",
    "        if theta.shape[0] != self.dim:\n",
    "            print(\"Incorrect size\")\n",
    "\n",
    "        yhat = torch.matmul(self.W,theta)\n",
    "\n",
    "        return torch.sum((yhat - self.y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(True)\n",
      "ipykernel_launcher:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(17.1534, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "problem = Inverse()\n",
    "\n",
    "\n",
    "theta = cuda_(Variable(torch.tensor(torch.rand(10))))\n",
    "\n",
    "print(theta.shape[0] == Variable(torch.tensor(10)))\n",
    "\n",
    "problem.get_loss(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function _VariableFunctionsClass.pow>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}