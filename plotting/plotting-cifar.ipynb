{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00000-8e4b689b-23f5-4aaf-a4ed-3c4ff25da5e1",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2cf9c8ee",
        "execution_millis": 1324,
        "execution_start": 1618593980621,
        "deepnote_cell_type": "code"
      },
      "source": "%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom mlfunctions import MNISTNet, cache, do_fit,fit_optimizer, MNISTLoss, w, Optimizer\nimport numpy as np\nimport torch.optim as optim\nimport glob\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as autograd\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\nimport multiprocessing\nimport os.path\nimport csv\nimport copy\nimport joblib\nfrom torchvision import datasets\nimport torchvision\nimport seaborn as sns; sns.set(color_codes=True)\nsns.set_style(\"white\")\nfrom pdb import set_trace as bp\nfrom meta_module import MetaLinear, MetaModule,MetaConv2d,MetaConvTranspose2d,MetaBatchNorm2d\nimport functools\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-31117b04-3f05-4834-a071-dcb0bc5b9c20",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e8142bf1",
        "execution_millis": 18,
        "execution_start": 1618593981948,
        "deepnote_cell_type": "code"
      },
      "source": "# !mkdir _cache\ncache = joblib.Memory(location='_cache', verbose=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## CIFAR-10 training \n\nFirst, change the optimizer structure using the MetaConv2D. ",
      "metadata": {
        "cell_id": "00002-243ecfea-5356-463f-93d4-e9c84688f8f6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-0824806f-73f1-49b3-9c55-4e89f1b68762",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "373ca267",
        "execution_millis": 22,
        "execution_start": 1618593981978,
        "deepnote_cell_type": "code"
      },
      "source": "class CIFAR10Loss:\n    def __init__(self, training=True):\n        dataset = datasets.CIFAR10(\n            '/datasets/cifar10', train=True, download=False,\n            transform=torchvision.transforms.ToTensor()\n        )\n        indices = list(range(len(dataset)))\n        np.random.RandomState(10).shuffle(indices)\n        if training:\n            indices = indices[:len(indices) // 2]\n        else:\n            indices = indices[len(indices) // 2:]\n\n        self.loader = torch.utils.data.DataLoader(\n            dataset, batch_size=128,\n            sampler=torch.utils.data.sampler.SubsetRandomSampler(indices))\n\n        self.batches = []\n        self.cur_batch = 0\n        \n    def sample(self):\n        if self.cur_batch >= len(self.batches):\n            self.batches = []\n            self.cur_batch = 0\n            for b in self.loader:\n                self.batches.append(b)\n        batch = self.batches[self.cur_batch]\n        self.cur_batch += 1\n        return batch\n\nclass CIFAR10Net(MetaModule):\n\n    def __init__(self, layer_size=32, n_layers=3, **kwargs):\n        super().__init__()\n\n\n        # add linear layers \n        \n        # for i in range(n_layers):\n        #     self.layers[f'mat_{i}'] = MetaLinear(inp_size, layer_size)\n        #     inp_size = layer_size\n\n        self.layers = {}\n        # Main layers (Convolutions + MaxPooling)\n        in_channels = 3\n        self.hidden_channels = [3,5,8,16]\n\n        for i in range(1,n_layers+1):\n            self.layers[f'conv_{i}'] = MetaConv2d(in_channels, self.hidden_channels[i], kernel_size=3)\n            self.layers[f'norm_{i}'] = nn.BatchNorm2d(self.hidden_channels[i-1], affine=False, track_running_stats=False)\n            in_channels = self.hidden_channels[i]\n\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=2,)\n        \n\n        # Linear Layers \n        self.layers['linear'] = MetaLinear(16*2*2 , layer_size)\n\n        self.layers = nn.ModuleDict(self.layers)\n\n        self.loss = nn.NLLLoss()\n\n    def all_named_parameters(self):\n        return [(k, v) for k, v in self.named_parameters()]\n    \n    def forward(self, loss):\n        inp, out = loss.sample()\n        inp = w(Variable(inp.view(inp.size()[0], 3,32,32)))    # reshapes from inp.shape\n        out = w(Variable(out))\n\n\n        for cur_layer in range(1,4):    #since the layers are defined from 1...\n            # print(\"Shape before conv2D - kernel = 3 \\t\" + str(inp.shape))\n            inp = self.layers[f'norm_{cur_layer}'](inp)    #affine = False makes the batch nor parameters not-learnable (keep it simple)\n            inp = self.layers[f'conv_{cur_layer}'](inp)\n            inp = self.pool(inp)\n            inp = self.activation(inp)\n            \n            cur_layer += 1\n            \n        # print(\"Shape after CNN \\t\" + str(inp.shape))\n        # inp = inp.view(-1, 16*2*2)  # flatten\n        inp = torch.flatten(inp,start_dim=1)\n        \n        inp = F.log_softmax(self.layers['linear'](inp), dim=1)\n\n        #debugging....\n        # print(inp.shape,out.shape)\n        l = self.loss(inp, out)\n\n        return l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "After the optmizee is done, we re-build the optmizer with athe new architecture: 2 LSTMs in series - one forthe conv2Dand one for the lienear (i.e.) fully connected. ",
      "metadata": {
        "cell_id": "00004-8e556b4c-65df-4796-9c7f-5bc0a9941d3d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-a517d98f-6d52-45f1-9ed3-831206e5246a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1feef430",
        "execution_millis": 20,
        "execution_start": 1618593982005,
        "deepnote_cell_type": "code"
      },
      "source": "\nclass Optimizer(nn.Module):\n    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n        super().__init__()\n        self.hidden_sz = hidden_sz\n        if preproc:\n            self.recurs = nn.LSTMCell(2, hidden_sz)\n        else:\n            self.recurs = nn.LSTMCell(1, hidden_sz)\n        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n        self.output = nn.Linear(hidden_sz, 1)\n        self.preproc = preproc\n        self.preproc_factor = preproc_factor\n        self.preproc_threshold = np.exp(-preproc_factor)\n\n    def forward(self, inp, hidden, cell):\n        if self.preproc:\n            # Implement preproc described in Appendix A\n\n            # Note: we do all this work on tensors, which means\n            # the gradients won't propagate through inp. This\n            # should be ok because the algorithm involves\n            # making sure that inp is already detached.\n            inp = inp.data\n            inp2 = w(torch.zeros(inp.size()[0], 2))\n            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n\n            inp2[:, 0][~keep_grads] = -1\n            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n            inp = w(Variable(inp2))\n        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n        \n        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now let's try to fit CIFAR classifier.",
      "metadata": {
        "cell_id": "00006-eca39226-0ed1-441e-9c41-a43c14be7dfc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00007-f87aad8c-8e0f-4d41-a498-d8a6e5b05ea7",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b917870f",
        "execution_millis": 19,
        "execution_start": 1618593982030,
        "deepnote_cell_type": "code"
      },
      "source": "def get_fit_dict_test(n_tests, opt_dict, *args, **kwargs):\n    opt = w(Optimizer(preproc=True))\n    # print(opt_dict)\n    # opt.load_state_dict(opt_dict)\n    np.random.seed(0)\n    return [do_fit(opt, *args, **kwargs) for _ in tqdm(range(N_TESTS), 'optimizer')]\n\n\n\ndef fit_normal(target_cls, target_to_opt, opt_class, n_tests=100, n_epochs=100, **kwargs):\n    results = []\n    for i in tqdm(range(n_tests), 'tests'):\n        target = target_cls(training=False)\n        optimizee = w(target_to_opt())\n        optimizer = opt_class(optimizee.parameters(), **kwargs)\n        total_loss = []\n        for _ in range(n_epochs):\n            loss = optimizee(target)\n            \n            total_loss.append(loss.data.cpu().numpy())\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        results.append(total_loss)\n\n        \n    return results",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Plotting",
      "metadata": {
        "tags": [],
        "cell_id": "00011-81f82c3e-a944-439c-b930-58deb76da422",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-8449e178-adcd-4e5a-a0f7-c58497e4c668",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e18d109d",
        "execution_millis": 4121,
        "execution_start": 1618593982096,
        "deepnote_cell_type": "code"
      },
      "source": "import torch.optim as optim\nimport torchvision as tv\n\ndataset = tv.datasets.CIFAR10(\n            '/datasets/cifar10', train=True, download=True,\n            transform=tv.transforms.ToTensor()\n        )\n\n\ndevice = torch.device('cpu')\nmnist_optimizer = torch.load('mnist_model.pt', map_location=device)",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Files already downloaded and verified\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-37dea388-637d-411e-a818-c85e8f76f2f9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5f5b2327",
        "execution_millis": 771283,
        "output_cleared": false,
        "execution_start": 1618593986223,
        "deepnote_cell_type": "code"
      },
      "source": "\nNORMAL_OPTS = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {'momentum': 0.9}), (optim.SGD, {'nesterov': True, 'momentum': 0.9})]\nOPT_NAMES = ['ADAM', 'RMSprop', 'SGD', 'NAG']\nLEARNING_RATES = [0.01, 0.003, 0.03, 0.01]\nN_TESTS = 50\nN_EPOCHS = 50\n\nfit_data = np.zeros((N_TESTS, N_EPOCHS, len(OPT_NAMES) + 1))\nfor i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, LEARNING_RATES)):\n    np.random.seed(0)\n    fit_data[:, :, i] = np.array(fit_normal(CIFAR10Loss, CIFAR10Net, opt, lr=lr, n_tests=N_TESTS, n_epochs=N_EPOCHS, **extra_kwargs))\n\n",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "tests: 100%|██████████| 50/50 [03:14<00:00,  3.89s/it]\ntests: 100%|██████████| 50/50 [03:12<00:00,  3.85s/it]\ntests: 100%|██████████| 50/50 [03:13<00:00,  3.87s/it]\ntests: 100%|██████████| 50/50 [03:10<00:00,  3.81s/it]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-684191af-e526-4173-90d2-dfb1e0415e5b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "ab938727",
        "execution_millis": 232176,
        "execution_start": 1618594757505,
        "deepnote_cell_type": "code"
      },
      "source": "fit_data[:, :, len(OPT_NAMES)] = np.array(get_fit_dict_test(N_TESTS, mnist_optimizer, None, CIFAR10Loss, CIFAR10Net, N_TESTS, N_EPOCHS, N_EPOCHS, out_mul=0.1, should_train=False))",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "optimizer: 100%|██████████| 50/50 [03:51<00:00,  4.64s/it]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-2280a237-1219-40c9-9574-6bd6415b45ac",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8c971823",
        "execution_millis": 183,
        "execution_start": 1618598181972,
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\ncondition=OPT_NAMES + ['LSTM']\ndef to_long_form(data_arr, condition):\n    records = []\n    for i in range(data_arr.shape[0]):\n        for j in range(data_arr.shape[1]):\n            for k in range(data_arr.shape[2]):\n                records.append((i, j, condition[k], data_arr[i, j, k]))\n    df = pd.DataFrame.from_records(records, columns=['test', 'Steps', 'Optimizer', 'Loss'] )\n    return df\n\nCOLORS = [\"red\", \"blue\", \"green\", \"purple\", \"orange\"]\n\ndf = to_long_form(fit_data, condition)\naxis = sns.lineplot(data=df, x='Steps', y='Loss', hue='Optimizer', ci=95, palette=COLORS)\nplt.gcf().set_figwidth(11.7)\n\nplt.yscale('log')\nplt.title(\"CIFAR10\")\nplt.show()",
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'OPT_NAMES' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46c93491a9b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPT_NAMES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_long_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OPT_NAMES' is not defined"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "np.save('mnist-on-cifar.npy', fit_data)",
      "metadata": {
        "tags": [],
        "cell_id": "00013-9de6dfc8-5810-482c-a85a-20cb09a54b18",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=b20e1b0b-d80b-4297-abae-d3a4799140e7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "deepnote_notebook_id": "b536eca2-bee4-4944-8fac-3be78a3856d3",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}